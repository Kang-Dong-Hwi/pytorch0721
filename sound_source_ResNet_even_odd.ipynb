{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BOn5xCyhP_c4",
    "outputId": "cc532aac-2858-440e-866d-01c2eac021c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numba.decorators\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numba.decorators import jit as optional_jit\n",
    "\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "\n",
    "#PATH = 'C://Projects//keras_talk//keras//intern//dataset//'\n",
    "PATH = '/content/gdrive/My Drive/dataset/'\n",
    "\n",
    "train_size = 800\n",
    "test_size = 200\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 40\n",
    "\n",
    "\n",
    "def Y_DATA(y_data):\n",
    "    for idx in range(y_data.shape[0]):\n",
    "        y = y_data[idx]\n",
    "        if y < 0:  y_data[idx] = 10\n",
    "        else:      y_data[idx] = (y//20)\n",
    "    return y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cLBucxEyDgyf"
   },
   "source": [
    "##### data normalization\n",
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Rk41xrd0XNHq",
    "outputId": "4f4f071a-247d-4f18-d2cc-7bd056925b89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dataset_dict = { 0 : 'S_left',        1 : 'S_left_phase',\n",
    "                 2 : 'S_right',       3 : 'S_right_phase',\n",
    "                 4 : 'clean_left',    5 : 'clean_left_phase',\n",
    "                 6 : 'clean_right',   7 : 'clean_right_phase',\n",
    "                 8 : 'idx_drone_end', 9 : 'idx_voice_end',\n",
    "                10 : 'idx_voice_start'}\n",
    "\n",
    "\n",
    "x_data_list = [0,2,1,3]\n",
    "\n",
    "\n",
    "numpy_dict = dict()\n",
    "for n in x_data_list:\n",
    "    numpy_name    = dataset_dict[n]\n",
    "    numpy_dict[n] = np.load( PATH + numpy_name + '.npy' )\n",
    "    \n",
    "\n",
    "\n",
    "'''    x_data,       y_data '''\n",
    "'''(1000,6,257,382), (1000,)'''\n",
    "\n",
    "x_data = []\n",
    "for idx in range(1000):\n",
    "    x_element = []\n",
    "\n",
    "    for n in x_data_list:\n",
    "        x_element.append( numpy_dict[n][:,:,idx] )\n",
    "\n",
    "   \n",
    "    \n",
    "    # log scale 변환 [dB]\n",
    "    x_L = numpy_dict[0][:,:,idx]\n",
    "    x_R = numpy_dict[2][:,:,idx]\n",
    "\n",
    "    x_L = 20*np.log10( np.abs(x_L) + np.finfo(np.float32).eps )\n",
    "    x_R = 20*np.log10( np.abs(x_R) + np.finfo(np.float32).eps )\n",
    "\n",
    "    # even mode, odd mode \n",
    "    #x_even = (x_L + x_R)/2\n",
    "    x_odd  = (x_L - x_R)/2\n",
    "\n",
    "\n",
    "    #x_element.append(x_even)\n",
    "    x_element.append(x_odd)\n",
    "    x_element = np.asarray( x_element )\n",
    "\n",
    "\n",
    "\n",
    "    # normalization\n",
    "    for k in range(len(x_element)):\n",
    "        x_mean = x_element[k].mean()\n",
    "        x_stdv = x_element[k].std()\n",
    "        x_element[k] = ( (x_element[k] - x_mean ) / x_stdv)\n",
    "    \n",
    "\n",
    "    x_data.append( x_element )\n",
    "\n",
    "\n",
    "x_data = np.asarray(x_data)\n",
    "y_data = Y_DATA( np.load(PATH + 'angle.npy') )\n",
    "print('done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "oH6kABe5XhZT"
   },
   "outputs": [],
   "source": [
    "\n",
    "#x_data = x_data.reshape()\n",
    "#y_data = y_data.reshape()\n",
    "\n",
    "\n",
    "\n",
    "x_data = torch.from_numpy( x_data ).float().to('cuda')\n",
    "y_data = torch.from_numpy( y_data ).long().to('cuda')\n",
    "\n",
    "full_dataset = TensorDataset( x_data, y_data )\n",
    "\n",
    "\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split( full_dataset, [train_size, test_size])\n",
    "train_dataset = DataLoader( dataset=train_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "valid_dataset = DataLoader( dataset=valid_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PjH37Dc-iHv7"
   },
   "source": [
    "#### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x3Qt8_mI1vY0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from torch import Tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LF-c1RmfObBr"
   },
   "source": [
    "#### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7uk0XRe_OY7g"
   },
   "outputs": [],
   "source": [
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        \n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        \n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        \n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x) # 1x1 stride = 1\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out) # 3x3 stride = stride\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out) # 1x1 planes, plaines * self.expansion\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=11, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        \n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        \n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        \n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        \n",
    "        \n",
    "\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "\n",
    "        # input : 4 x 257 x 382\n",
    "        self.conv1 = nn.Conv2d(5, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        # output = self.conv1( input )\n",
    "        # output : 64 x 129 x 191\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # input : 64 x 65 x 86\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # output : 64*65*86\n",
    "\n",
    "        #[layers] = [3,4,6,3]\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    \n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "\n",
    "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def resnet_custom(pretrained=False, progress=True, **kwargs):\n",
    "\n",
    "    return _resnet('resnet101', Bottleneck, [3, 5, 5, 3], pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q8hnA7UlDTeG"
   },
   "source": [
    "##### model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LCRi61pl6_pY",
    "outputId": "56787455-4e51-4576-c104-d9738400addb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1\n",
      "\tLoss: 0.068\n",
      "\tLoss: -0.069\n",
      "\tLoss: -0.058\n",
      "\tLoss: -0.093\n",
      "\tLoss: -0.071\n",
      "   Acc: 9.375\n",
      "epoch2\n",
      "\tLoss: -0.114\n",
      "\tLoss: -0.078\n",
      "\tLoss: -0.144\n",
      "\tLoss: -0.070\n",
      "\tLoss: -0.182\n",
      "   Acc: 9.938\n",
      "epoch3\n",
      "\tLoss: -0.175\n",
      "\tLoss: -0.217\n",
      "\tLoss: -0.152\n",
      "\tLoss: -0.188\n",
      "\tLoss: -0.207\n",
      "   Acc: 10.292\n",
      "epoch4\n",
      "\tLoss: -0.347\n",
      "\tLoss: -0.159\n",
      "\tLoss: -0.436\n",
      "\tLoss: -0.323\n",
      "\tLoss: -0.301\n",
      "   Acc: 10.719\n",
      "epoch5\n",
      "\tLoss: -0.266\n",
      "\tLoss: -0.388\n",
      "\tLoss: -0.322\n",
      "\tLoss: -0.332\n",
      "\tLoss: -0.241\n",
      "   Acc: 11.025\n",
      "epoch6\n",
      "\tLoss: -0.340\n",
      "\tLoss: -0.434\n",
      "\tLoss: -0.427\n",
      "\tLoss: -0.427\n",
      "\tLoss: -0.385\n",
      "   Acc: 11.167\n",
      "epoch7\n",
      "\tLoss: -0.369\n",
      "\tLoss: -0.403\n",
      "\tLoss: -0.484\n",
      "\tLoss: -0.321\n",
      "\tLoss: -0.344\n",
      "   Acc: 11.500\n",
      "epoch8\n",
      "\tLoss: -0.431\n",
      "\tLoss: -0.453\n",
      "\tLoss: -0.472\n",
      "\tLoss: -0.462\n",
      "\tLoss: -0.437\n",
      "   Acc: 11.688\n",
      "epoch9\n",
      "\tLoss: -0.395\n",
      "\tLoss: -0.499\n",
      "\tLoss: -0.411\n",
      "\tLoss: -0.537\n",
      "\tLoss: -0.542\n",
      "   Acc: 11.889\n",
      "epoch10\n",
      "\tLoss: -0.577\n",
      "\tLoss: -0.477\n",
      "\tLoss: -0.572\n",
      "\tLoss: -0.473\n",
      "\tLoss: -0.618\n",
      "   Acc: 12.025\n",
      "epoch11\n",
      "\tLoss: -0.574\n",
      "\tLoss: -0.695\n",
      "\tLoss: -0.554\n",
      "\tLoss: -0.550\n",
      "\tLoss: -0.602\n",
      "   Acc: 12.170\n",
      "epoch12\n",
      "\tLoss: -0.708\n",
      "\tLoss: -0.678\n",
      "\tLoss: -0.579\n",
      "\tLoss: -0.611\n",
      "\tLoss: -0.698\n",
      "   Acc: 12.219\n",
      "epoch13\n",
      "\tLoss: -0.725\n",
      "\tLoss: -0.625\n",
      "\tLoss: -0.640\n",
      "\tLoss: -0.585\n",
      "\tLoss: -0.657\n",
      "   Acc: 12.269\n",
      "epoch14\n",
      "\tLoss: -0.495\n",
      "\tLoss: -0.651\n",
      "\tLoss: -0.628\n",
      "\tLoss: -0.659\n",
      "\tLoss: -0.680\n",
      "   Acc: 12.304\n",
      "epoch15\n",
      "\tLoss: -0.645\n",
      "\tLoss: -0.630\n",
      "\tLoss: -0.669\n",
      "\tLoss: -0.691\n",
      "\tLoss: -0.675\n",
      "   Acc: 12.358\n",
      "epoch16\n",
      "\tLoss: -0.774\n",
      "\tLoss: -0.733\n",
      "\tLoss: -0.651\n",
      "\tLoss: -0.729\n",
      "\tLoss: -0.705\n",
      "   Acc: 12.445\n",
      "epoch17\n",
      "\tLoss: -0.745\n",
      "\tLoss: -0.634\n",
      "\tLoss: -0.781\n",
      "\tLoss: -0.728\n",
      "\tLoss: -0.749\n",
      "   Acc: 12.515\n",
      "epoch18\n",
      "\tLoss: -0.784\n",
      "\tLoss: -0.689\n",
      "\tLoss: -0.734\n",
      "\tLoss: -0.766\n",
      "\tLoss: -0.821\n",
      "   Acc: 12.604\n",
      "epoch19\n",
      "\tLoss: -0.777\n",
      "\tLoss: -0.842\n",
      "\tLoss: -0.663\n",
      "\tLoss: -0.816\n",
      "\tLoss: -0.761\n",
      "   Acc: 12.704\n",
      "epoch20\n",
      "\tLoss: -0.759\n",
      "\tLoss: -0.786\n",
      "\tLoss: -0.806\n",
      "\tLoss: -0.763\n",
      "\tLoss: -0.817\n",
      "   Acc: 12.812\n",
      "epoch21\n",
      "\tLoss: -0.871\n",
      "\tLoss: -0.745\n",
      "\tLoss: -0.822\n",
      "\tLoss: -0.819\n",
      "\tLoss: -0.848\n",
      "   Acc: 12.923\n",
      "epoch22\n",
      "\tLoss: -0.898\n",
      "\tLoss: -0.842\n",
      "\tLoss: -0.837\n",
      "\tLoss: -0.973\n",
      "\tLoss: -0.951\n",
      "   Acc: 12.966\n",
      "epoch23\n",
      "\tLoss: -0.836\n",
      "\tLoss: -0.816\n",
      "\tLoss: -0.867\n",
      "\tLoss: -0.876\n",
      "\tLoss: -0.927\n",
      "   Acc: 13.054\n",
      "epoch24\n",
      "\tLoss: -1.019\n",
      "\tLoss: -0.883\n",
      "\tLoss: -0.931\n",
      "\tLoss: -0.863\n",
      "\tLoss: -0.903\n",
      "   Acc: 13.109\n",
      "epoch25\n",
      "\tLoss: -0.990\n",
      "\tLoss: -0.985\n",
      "\tLoss: -0.874\n",
      "\tLoss: -0.860\n",
      "\tLoss: -1.029\n",
      "   Acc: 13.110\n",
      "epoch26\n",
      "\tLoss: -0.952\n",
      "\tLoss: -1.060\n",
      "\tLoss: -0.925\n",
      "\tLoss: -0.944\n",
      "\tLoss: -0.938\n",
      "   Acc: 13.178\n",
      "epoch27\n",
      "\tLoss: -0.860\n",
      "\tLoss: -0.962\n",
      "\tLoss: -1.085\n",
      "\tLoss: -0.946\n",
      "\tLoss: -1.005\n",
      "   Acc: 13.208\n",
      "epoch28\n",
      "\tLoss: -0.985\n",
      "\tLoss: -1.011\n",
      "\tLoss: -0.925\n",
      "\tLoss: -0.964\n",
      "\tLoss: -0.933\n",
      "   Acc: 13.237\n",
      "epoch29\n",
      "\tLoss: -1.018\n",
      "\tLoss: -1.002\n",
      "\tLoss: -1.019\n",
      "\tLoss: -0.911\n",
      "\tLoss: -1.006\n",
      "   Acc: 13.310\n",
      "epoch30\n",
      "\tLoss: -0.965\n",
      "\tLoss: -0.938\n",
      "\tLoss: -1.025\n",
      "\tLoss: -1.020\n",
      "\tLoss: -1.131\n",
      "   Acc: 13.371\n",
      "epoch31\n",
      "\tLoss: -1.024\n",
      "\tLoss: -1.000\n",
      "\tLoss: -1.013\n",
      "\tLoss: -1.000\n",
      "\tLoss: -1.120\n",
      "   Acc: 13.399\n",
      "epoch32\n",
      "\tLoss: -1.061\n",
      "\tLoss: -0.960\n",
      "\tLoss: -1.120\n",
      "\tLoss: -0.977\n",
      "\tLoss: -1.096\n",
      "   Acc: 13.473\n",
      "epoch33\n",
      "\tLoss: -1.008\n",
      "\tLoss: -1.126\n",
      "\tLoss: -1.027\n",
      "\tLoss: -1.101\n",
      "\tLoss: -1.173\n",
      "   Acc: 13.511\n",
      "epoch34\n",
      "\tLoss: -1.154\n",
      "\tLoss: -1.088\n",
      "\tLoss: -1.132\n",
      "\tLoss: -1.105\n",
      "\tLoss: -1.076\n",
      "   Acc: 13.562\n",
      "epoch35\n",
      "\tLoss: -1.140\n",
      "\tLoss: -1.102\n",
      "\tLoss: -1.157\n",
      "\tLoss: -1.218\n",
      "\tLoss: -1.220\n",
      "   Acc: 13.600\n",
      "epoch36\n",
      "\tLoss: -1.114\n",
      "\tLoss: -1.093\n",
      "\tLoss: -1.090\n",
      "\tLoss: -1.086\n",
      "\tLoss: -1.223\n",
      "   Acc: 13.628\n",
      "epoch37\n",
      "\tLoss: -1.097\n",
      "\tLoss: -1.149\n",
      "\tLoss: -1.137\n",
      "\tLoss: -1.184\n",
      "\tLoss: -1.171\n",
      "   Acc: 13.642\n",
      "epoch38\n",
      "\tLoss: -1.241\n",
      "\tLoss: -1.277\n",
      "\tLoss: -1.109\n",
      "\tLoss: -1.150\n",
      "\tLoss: -1.093\n",
      "   Acc: 13.681\n",
      "epoch39\n",
      "\tLoss: -1.178\n",
      "\tLoss: -1.137\n",
      "\tLoss: -1.041\n",
      "\tLoss: -1.154\n",
      "\tLoss: -1.106\n",
      "   Acc: 13.712\n",
      "epoch40\n",
      "\tLoss: -1.217\n",
      "\tLoss: -1.316\n",
      "\tLoss: -1.243\n",
      "\tLoss: -1.200\n",
      "\tLoss: -1.191\n",
      "   Acc: 13.756\n",
      "epoch41\n",
      "\tLoss: -1.242\n",
      "\tLoss: -1.209\n",
      "\tLoss: -1.151\n",
      "\tLoss: -1.238\n",
      "\tLoss: -1.267\n",
      "   Acc: 13.787\n",
      "epoch42\n",
      "\tLoss: -1.247\n",
      "\tLoss: -1.176\n",
      "\tLoss: -1.258\n",
      "\tLoss: -1.284\n",
      "\tLoss: -1.253\n",
      "   Acc: 13.824\n",
      "epoch43\n",
      "\tLoss: -1.192\n",
      "\tLoss: -1.178\n",
      "\tLoss: -1.354\n",
      "\tLoss: -1.306\n",
      "\tLoss: -1.344\n",
      "   Acc: 13.860\n",
      "epoch44\n",
      "\tLoss: -1.244\n",
      "\tLoss: -1.296\n",
      "\tLoss: -1.138\n",
      "\tLoss: -1.352\n",
      "\tLoss: -1.222\n",
      "   Acc: 13.906\n",
      "epoch45\n",
      "\tLoss: -1.278\n",
      "\tLoss: -1.366\n",
      "\tLoss: -1.220\n",
      "\tLoss: -1.317\n",
      "\tLoss: -1.270\n",
      "   Acc: 13.936\n",
      "epoch46\n",
      "\tLoss: -1.214\n",
      "\tLoss: -1.387\n",
      "\tLoss: -1.372\n",
      "\tLoss: -1.227\n",
      "\tLoss: -1.262\n",
      "   Acc: 13.959\n",
      "epoch47\n",
      "\tLoss: -1.325\n",
      "\tLoss: -1.296\n",
      "\tLoss: -1.251\n",
      "\tLoss: -1.333\n",
      "\tLoss: -1.315\n",
      "   Acc: 13.997\n",
      "epoch48\n",
      "\tLoss: -1.407\n",
      "\tLoss: -1.309\n",
      "\tLoss: -1.302\n",
      "\tLoss: -1.380\n",
      "\tLoss: -1.256\n",
      "   Acc: 14.034\n",
      "epoch49\n",
      "\tLoss: -1.439\n",
      "\tLoss: -1.337\n",
      "\tLoss: -1.315\n",
      "\tLoss: -1.327\n",
      "\tLoss: -1.278\n",
      "   Acc: 14.066\n",
      "epoch50\n",
      "\tLoss: -1.260\n",
      "\tLoss: -1.484\n",
      "\tLoss: -1.379\n",
      "\tLoss: -1.312\n",
      "\tLoss: -1.334\n",
      "   Acc: 14.102\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "#model = densenet_custom().to('cuda')\n",
    "model = resnet_custom().to('cuda')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to('cuda')\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.00001, weight_decay=0.9)\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.00001)\n",
    "\n",
    "total=0\n",
    "corr=0\n",
    "model.train()\n",
    "for epoch in range(50):\n",
    "    print('epoch' + str(epoch+1))\n",
    "    \n",
    "    for i, (data, label) in enumerate(train_dataset):\n",
    "        (data, label) = (data.to('cuda'), label.to('cuda'))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "  \n",
    "        loss = F.nll_loss(output, label.reshape(BATCH_SIZE))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        preds = output.data.max(1)[1]\n",
    "        total += BATCH_SIZE\n",
    "        corr  += (preds==label.reshape(BATCH_SIZE)).sum().item()\n",
    "        \n",
    "\n",
    "        if i%4 == 0: print('\\tLoss: {:.3f}'.format(loss.item()))\n",
    "    print('   Acc: {:.3f}'.format( 100*corr/total ))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oFJ9r6gFVG05",
    "outputId": "3bb1187f-299f-4c39-f723-2f41fd24f15b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  7.0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, label in valid_dataset:\n",
    "        output = model(data)\n",
    "        preds  = torch.max(output.data, 1)[1]\n",
    "        total   += len(label)\n",
    "        \n",
    "        label = label.reshape(BATCH_SIZE)\n",
    "        correct += (preds==label).sum().item()\n",
    "      \n",
    "    print('Test Accuracy: ', 100.*correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dzOv6dQtKC6k"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sound_source_ResNet_even_odd.ipynb ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
