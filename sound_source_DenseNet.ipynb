{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sound_source_DenseNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BOn5xCyhP_c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a17aa6cb-4ef1-481d-d9e6-377c31035d80"
      },
      "source": [
        "\n",
        "import torch\n",
        "import os\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numba.decorators\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numba.decorators import jit as optional_jit\n",
        "\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "#PATH = 'C://Projects//keras_talk//keras//intern//dataset//'\n",
        "PATH = '/content/gdrive/My Drive/dataset/'\n",
        "\n",
        "train_size = 800\n",
        "test_size = 200\n",
        "\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 40\n",
        "\n",
        "\n",
        "def Y_DATA(y_data):\n",
        "    for idx in range(y_data.shape[0]):\n",
        "        y = y_data[idx]\n",
        "        if y < 0:  y_data[idx] = 10\n",
        "        else:      y_data[idx] = (y//20)\n",
        "    return y_data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLBucxEyDgyf",
        "colab_type": "text"
      },
      "source": [
        "###### data normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rk41xrd0XNHq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58664ce6-5a04-43c2-b405-c63a207ad4bd"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "dataset_dict = { 0 : 'S_left',        1 : 'S_left_phase',\n",
        "                 2 : 'S_right',       3 : 'S_right_phase',\n",
        "                 4 : 'clean_left',    5 : 'clean_left_phase',\n",
        "                 6 : 'clean_right',   7 : 'clean_right_phase',\n",
        "                 8 : 'idx_drone_end', 9 : 'idx_voice_end',\n",
        "                10 : 'idx_voice_start'}\n",
        "\n",
        "\n",
        "x_data_list = [0,2,1,3]\n",
        "\n",
        "\n",
        "numpy_dict = dict()\n",
        "for n in x_data_list:\n",
        "    numpy_name    = dataset_dict[n]\n",
        "    numpy_dict[n] = np.load( PATH + numpy_name + '.npy' )\n",
        "    \n",
        "\n",
        "\n",
        "'''    x_data,       y_data '''\n",
        "'''(1000,6,257,382), (1000,)'''\n",
        "\n",
        "x_data = []\n",
        "for idx in range(1000):\n",
        "    x_element = []\n",
        "\n",
        "    for n in x_data_list:\n",
        "        x_element.append( numpy_dict[n][:,:,idx] )\n",
        "\n",
        "   \n",
        "    \n",
        "    # log scale 변환 [dB]\n",
        "    x_L = numpy_dict[0][:,:,idx]\n",
        "    x_R = numpy_dict[2][:,:,idx]\n",
        "\n",
        "    x_L = 20*np.log10( np.abs(x_L) + np.finfo(np.float32).eps )\n",
        "    x_R = 20*np.log10( np.abs(x_R) + np.finfo(np.float32).eps )\n",
        "\n",
        "    # even mode, odd mode \n",
        "    #x_even = (x_L + x_R)/2\n",
        "    x_odd  = (x_L - x_R)/2\n",
        "\n",
        "\n",
        "    #x_element.append(x_even)\n",
        "    x_element.append(x_odd)\n",
        "    x_element = np.asarray( x_element )\n",
        "\n",
        "\n",
        "\n",
        "    # normalization\n",
        "    for k in range(len(x_element)):\n",
        "        x_mean = x_element[k].mean()\n",
        "        x_stdv = x_element[k].std()\n",
        "        x_element[k] = ( (x_element[k] - x_mean ) / x_stdv)\n",
        "    \n",
        "\n",
        "    x_data.append( x_element )\n",
        "\n",
        "\n",
        "x_data = np.asarray(x_data)\n",
        "y_data = Y_DATA( np.load(PATH + 'angle.npy') )\n",
        "print('done..')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab_type": "code",
        "id": "oH6kABe5XhZT",
        "colab": {}
      },
      "source": [
        "\n",
        "#x_data = x_data.reshape()\n",
        "#y_data = y_data.reshape()\n",
        "\n",
        "\n",
        "\n",
        "x_data = torch.from_numpy( x_data ).float().to('cuda')\n",
        "y_data = torch.from_numpy( y_data ).long().to('cuda')\n",
        "\n",
        "full_dataset = TensorDataset( x_data, y_data )\n",
        "\n",
        "\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split( full_dataset, [train_size, test_size])\n",
        "train_dataset = DataLoader( dataset=train_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "valid_dataset = DataLoader( dataset=valid_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last=True)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PjH37Dc-iHv7"
      },
      "source": [
        "#### DenseNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3Qt8_mI1vY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "from torch import Tensor\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf8k2LB5Ug83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class _DenseLayer(nn.Module):\n",
        "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n",
        "        super(_DenseLayer, self).__init__()\n",
        "        \n",
        "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
        "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
        "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size * growth_rate,\n",
        "                                           kernel_size=1, stride=1,bias=False)),\n",
        "        \n",
        "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
        "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
        "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
        "                                           kernel_size=3, stride=1, padding=1,\n",
        "                                           bias=False)),\n",
        "        \n",
        "        self.drop_rate = float(drop_rate)\n",
        "        self.memory_efficient = memory_efficient\n",
        "\n",
        "    def bn_function(self, inputs):\n",
        "        # type: (List[Tensor]) -> Tensor\n",
        "        concated_features = torch.cat(inputs, 1)\n",
        "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))  # noqa: T484\n",
        "        \n",
        "        return bottleneck_output\n",
        "\n",
        "\n",
        "    def forward(self, input):  # noqa: F811\n",
        "        if isinstance(input, Tensor):\n",
        "            prev_features = [input]\n",
        "        else:\n",
        "            prev_features = input\n",
        "\n",
        "\n",
        "        bottleneck_output = self.bn_function(prev_features)\n",
        "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
        "        \n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
        "                                     training=self.training)\n",
        "        \n",
        "        return new_features\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj_EnuGyUhfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class _DenseBlock(nn.ModuleDict):\n",
        "    _version = 2\n",
        "\n",
        "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n",
        "        super(_DenseBlock, self).__init__()\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(\n",
        "                num_input_features + i * growth_rate,\n",
        "                growth_rate=growth_rate,\n",
        "                bn_size=bn_size,\n",
        "                drop_rate=drop_rate,\n",
        "                memory_efficient=memory_efficient,\n",
        "            )\n",
        "            self.add_module('denselayer%d' % (i + 1), layer)\n",
        "\n",
        "    def forward(self, init_features):\n",
        "        features = [init_features]\n",
        "        for name, layer in self.items():\n",
        "            new_features = layer(features)\n",
        "            features.append(new_features)\n",
        "        return torch.cat(features, 1)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGPf02PMUh4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class _Transition(nn.Sequential):\n",
        "    def __init__(self, num_input_features, num_output_features):\n",
        "        super(_Transition, self).__init__()\n",
        "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
        "        self.add_module('relu', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv', nn.Conv2d(num_input_features,\n",
        "                                          num_output_features,\n",
        "                                          kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26XC_s-lUtwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    '''growth_rate, drop_rate'''\n",
        "    def __init__(self, growth_rate=20, block_config=(6, 12, 24, 16),\n",
        "                 num_init_features=128, bn_size=4, drop_rate=0.10,\n",
        "                 num_classes=11, memory_efficient=False):\n",
        "\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        # First convolution\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv0', nn.Conv2d(5, num_init_features, kernel_size=3, stride=2, padding=1, bias=False)),\n",
        "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
        "            ('relu0', nn.ReLU(inplace=True)),\n",
        "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
        "        ]))\n",
        "\n",
        "\n",
        "        # Each denseblock\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            block = _DenseBlock(\n",
        "                num_layers=num_layers,\n",
        "                num_input_features=num_features,\n",
        "                bn_size=bn_size,\n",
        "                growth_rate=growth_rate,\n",
        "                drop_rate=drop_rate,\n",
        "                memory_efficient=memory_efficient\n",
        "            )\n",
        "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "\n",
        "            if i != len(block_config) - 1:\n",
        "                trans = _Transition(num_input_features=num_features,\n",
        "                                    num_output_features=num_features // 2)\n",
        "                self.features.add_module('transition%d' % (i + 1), trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
        "\n",
        "        # Linear layer\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "        # Official init from torch repo.\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        out = F.relu(features, inplace=True)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1EZpcTrU3qh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def _densenet(arch, growth_rate, block_config, num_init_features, pretrained, progress, **kwargs):\n",
        "    return DenseNet(growth_rate, block_config, num_init_features, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def densenet_custom(pretrained=False, progress=True, **kwargs):\n",
        "    return _densenet('densenet_custom', 10, (6, 6, 4), 128, pretrained, progress, **kwargs)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8hnA7UlDTeG",
        "colab_type": "text"
      },
      "source": [
        "##### model train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLyIBBFwNBvr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "644e1375-d542-44d3-83d4-590c9f6cc2f3"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "\n",
        "model = densenet_custom().to('cuda')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.00001, weight_decay=0.9)\n",
        "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.00001)\n",
        "\n",
        "total=0\n",
        "corr=0\n",
        "model.train()\n",
        "for epoch in range(50):\n",
        "    print('epoch' + str(epoch+1))\n",
        "    \n",
        "    for i, (data, label) in enumerate(train_dataset):\n",
        "        (data, label) = (data.to('cuda'), label.to('cuda'))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "  \n",
        "        loss = F.nll_loss(output, label.reshape(BATCH_SIZE))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        preds = output.data.max(1)[1]\n",
        "        total += BATCH_SIZE\n",
        "        corr  += (preds==label.reshape(BATCH_SIZE)).sum().item()\n",
        "        \n",
        "\n",
        "        if i%10 == 0: print('\\tLoss: {:.3f}'.format(loss.item()))\n",
        "    print('   Acc: {:.3f}'.format( 100*corr/total ))\n",
        "        "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch1\n",
            "\tLoss: 0.030\n",
            "\tLoss: 0.031\n",
            "   Acc: 11.875\n",
            "epoch2\n",
            "\tLoss: 0.039\n",
            "\tLoss: -0.001\n",
            "   Acc: 11.688\n",
            "epoch3\n",
            "\tLoss: 0.019\n",
            "\tLoss: 0.072\n",
            "   Acc: 11.667\n",
            "epoch4\n",
            "\tLoss: -0.021\n",
            "\tLoss: 0.034\n",
            "   Acc: 11.719\n",
            "epoch5\n",
            "\tLoss: -0.045\n",
            "\tLoss: -0.029\n",
            "   Acc: 11.675\n",
            "epoch6\n",
            "\tLoss: 0.000\n",
            "\tLoss: 0.006\n",
            "   Acc: 11.708\n",
            "epoch7\n",
            "\tLoss: 0.010\n",
            "\tLoss: -0.032\n",
            "   Acc: 11.661\n",
            "epoch8\n",
            "\tLoss: -0.002\n",
            "\tLoss: -0.001\n",
            "   Acc: 11.703\n",
            "epoch9\n",
            "\tLoss: -0.035\n",
            "\tLoss: 0.093\n",
            "   Acc: 11.708\n",
            "epoch10\n",
            "\tLoss: -0.044\n",
            "\tLoss: 0.032\n",
            "   Acc: 11.713\n",
            "epoch11\n",
            "\tLoss: 0.052\n",
            "\tLoss: 0.039\n",
            "   Acc: 11.761\n",
            "epoch12\n",
            "\tLoss: -0.002\n",
            "\tLoss: -0.010\n",
            "   Acc: 11.865\n",
            "epoch13\n",
            "\tLoss: -0.046\n",
            "\tLoss: -0.003\n",
            "   Acc: 11.904\n",
            "epoch14\n",
            "\tLoss: -0.028\n",
            "\tLoss: -0.022\n",
            "   Acc: 11.884\n",
            "epoch15\n",
            "\tLoss: -0.018\n",
            "\tLoss: -0.030\n",
            "   Acc: 11.900\n",
            "epoch16\n",
            "\tLoss: -0.060\n",
            "\tLoss: -0.010\n",
            "   Acc: 11.906\n",
            "epoch17\n",
            "\tLoss: -0.030\n",
            "\tLoss: -0.019\n",
            "   Acc: 11.912\n",
            "epoch18\n",
            "\tLoss: 0.022\n",
            "\tLoss: -0.042\n",
            "   Acc: 11.938\n",
            "epoch19\n",
            "\tLoss: -0.038\n",
            "\tLoss: -0.027\n",
            "   Acc: 11.961\n",
            "epoch20\n",
            "\tLoss: -0.031\n",
            "\tLoss: 0.031\n",
            "   Acc: 11.994\n",
            "epoch21\n",
            "\tLoss: -0.014\n",
            "\tLoss: -0.042\n",
            "   Acc: 11.994\n",
            "epoch22\n",
            "\tLoss: -0.049\n",
            "\tLoss: -0.001\n",
            "   Acc: 12.017\n",
            "epoch23\n",
            "\tLoss: -0.002\n",
            "\tLoss: -0.050\n",
            "   Acc: 12.033\n",
            "epoch24\n",
            "\tLoss: -0.011\n",
            "\tLoss: 0.014\n",
            "   Acc: 12.057\n",
            "epoch25\n",
            "\tLoss: -0.012\n",
            "\tLoss: -0.033\n",
            "   Acc: 12.080\n",
            "epoch26\n",
            "\tLoss: 0.001\n",
            "\tLoss: -0.018\n",
            "   Acc: 12.087\n",
            "epoch27\n",
            "\tLoss: -0.004\n",
            "\tLoss: -0.005\n",
            "   Acc: 12.106\n",
            "epoch28\n",
            "\tLoss: -0.044\n",
            "\tLoss: 0.016\n",
            "   Acc: 12.116\n",
            "epoch29\n",
            "\tLoss: -0.035\n",
            "\tLoss: -0.060\n",
            "   Acc: 12.112\n",
            "epoch30\n",
            "\tLoss: -0.023\n",
            "\tLoss: 0.018\n",
            "   Acc: 12.113\n",
            "epoch31\n",
            "\tLoss: -0.033\n",
            "\tLoss: 0.024\n",
            "   Acc: 12.133\n",
            "epoch32\n",
            "\tLoss: -0.003\n",
            "\tLoss: -0.054\n",
            "   Acc: 12.125\n",
            "epoch33\n",
            "\tLoss: -0.031\n",
            "\tLoss: 0.031\n",
            "   Acc: 12.133\n",
            "epoch34\n",
            "\tLoss: -0.084\n",
            "\tLoss: -0.018\n",
            "   Acc: 12.154\n",
            "epoch35\n",
            "\tLoss: -0.011\n",
            "\tLoss: 0.005\n",
            "   Acc: 12.154\n",
            "epoch36\n",
            "\tLoss: -0.031\n",
            "\tLoss: -0.033\n",
            "   Acc: 12.177\n",
            "epoch37\n",
            "\tLoss: -0.045\n",
            "\tLoss: -0.075\n",
            "   Acc: 12.189\n",
            "epoch38\n",
            "\tLoss: -0.021\n",
            "\tLoss: -0.025\n",
            "   Acc: 12.211\n",
            "epoch39\n",
            "\tLoss: -0.003\n",
            "\tLoss: -0.061\n",
            "   Acc: 12.218\n",
            "epoch40\n",
            "\tLoss: -0.035\n",
            "\tLoss: -0.029\n",
            "   Acc: 12.234\n",
            "epoch41\n",
            "\tLoss: -0.021\n",
            "\tLoss: -0.022\n",
            "   Acc: 12.241\n",
            "epoch42\n",
            "\tLoss: -0.057\n",
            "\tLoss: -0.035\n",
            "   Acc: 12.256\n",
            "epoch43\n",
            "\tLoss: -0.003\n",
            "\tLoss: -0.002\n",
            "   Acc: 12.262\n",
            "epoch44\n",
            "\tLoss: -0.051\n",
            "\tLoss: -0.024\n",
            "   Acc: 12.276\n",
            "epoch45\n",
            "\tLoss: -0.014\n",
            "\tLoss: 0.036\n",
            "   Acc: 12.297\n",
            "epoch46\n",
            "\tLoss: -0.027\n",
            "\tLoss: -0.060\n",
            "   Acc: 12.310\n",
            "epoch47\n",
            "\tLoss: 0.000\n",
            "\tLoss: -0.050\n",
            "   Acc: 12.311\n",
            "epoch48\n",
            "\tLoss: -0.047\n",
            "\tLoss: -0.014\n",
            "   Acc: 12.326\n",
            "epoch49\n",
            "\tLoss: -0.038\n",
            "\tLoss: -0.021\n",
            "   Acc: 12.349\n",
            "epoch50\n",
            "\tLoss: -0.019\n",
            "\tLoss: -0.042\n",
            "   Acc: 12.370\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzOv6dQtKC6k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d3f874b-d7c3-4c43-ee2e-689213dfe054"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data, label in valid_dataset:\n",
        "        output = model(data)\n",
        "        preds  = torch.max(output.data, 1)[1]\n",
        "        total   += len(label)\n",
        "        \n",
        "        label = label.reshape(BATCH_SIZE)\n",
        "        correct += (preds==label).sum().item()\n",
        "      \n",
        "    print('Test Accuracy: ', 100.*correct/total)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:  12.5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}